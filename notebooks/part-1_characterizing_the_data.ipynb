{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ecb309f7",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Peek My Home Price Part-1: Characterizing the data'\n",
    "author: Adam Cseresznye\n",
    "date: '2024-11-20'\n",
    "categories:\n",
    "  - Belgian Housing Market Insights\n",
    "jupyter: python3\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5a692-e95d-4785-8d85-ed1cc10b4056",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Photo by Christian Allard UnSplash](https://images.unsplash.com/photo-1549407408-4b016f497e4d?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D){fig-align=\"center\" width=50%}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afceccc-a50f-4596-a6fa-a511693421bd",
   "metadata": {},
   "source": [
    "Welcome to `Peek My Home Price`, a project, that dives into the key factors that influence real estate property prices in Belgium. Our ultimate goal with this project is to leverage up-to-date data from leading real estate platforms in the country to accurately predict house prices. We aim to create a platform that allows users to gain insights into the dynamic Belgian real estate market, province by province."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a779a6-8571-43b2-98c9-40b6d15f4729",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "You can explore the project's app on its [website](https://peek-my-home-price.fly.dev/). For more details, visit the [GitHub repository](https://github.com/adamcseresznye/peek_my_home_price).\n",
    "\n",
    "Check out the series for a deeper dive:\n",
    "- [Part 1: Characterizing the Data](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-1_characterizing_the_data.html)\n",
    "- [Part 2: Building a Baseline Model](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-2_building_a_baseline_model.html)\n",
    "- [Part 3: Feature Selection](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-3_feature_selection.html)\n",
    "- [Part 4: Feature Engineering](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-4_feature_engineering.html)\n",
    "- [Part 5: Fine-Tuning](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-5_fine_tuning.html)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d67c1-2cb1-43b9-a899-7794b6d238df",
   "metadata": {},
   "source": [
    "The app is divided into three main sections:\n",
    "\n",
    "1) **Dashboard**: Get a quick snapshot of the latest real estate trends, including average prices, the most active regions, and interesting facts about the dataset.\n",
    "2) **Trends**: Dive deeper into historical price trends. Explore median price changes over time for each Belgian province.\n",
    "3) **Prediction**: Input specific variables and generate your own price predictions based on our latest trained model.\n",
    "\n",
    "In this blog series, we'll take you behind the scenes of `Peek My Home Price` and guide you through the thought process that led to the creation of the application. Feel free to explore the topics that pique your interest or that you'd like to learn more about. We hope you'll find this information valuable for your own projects. Let's get started!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc5709-2469-4248-94b7-17d180d1b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf78a4-9f6c-4136-8d79-2c5dea88e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lets_plot import *\n",
    "from lets_plot.bistro.corr import *\n",
    "from lets_plot.mapping import as_discrete\n",
    "\n",
    "LetsPlot.setup_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757fbc9-bab2-4a16-96b3-713ab4947f33",
   "metadata": {},
   "source": [
    "\n",
    "# Web scraping\n",
    "\n",
    "The data used to train our model is sourced from prominent Belgian real estate platforms. Employing the `Scrapy` framework, we systematically extract relevant features. The collected data then undergoes a preprocessing pipeline, including duplicate removal and data type conversion. The cleaned dataset is subsequently stored in a `PostgreSQL` database, ready for model training. If you would like to learn more about this step, please visit the [`src/scraper`](https://github.com/adamcseresznye/peek_my_home_price/tree/main/src/scraper) module. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95cd67a-b04b-4343-b81a-704aa6bee205",
   "metadata": {},
   "source": [
    "# Describing the data\n",
    "\n",
    "Before diving into analysis, it's crucial to take a closer look at our dataset's preliminary version. This step is essential because, often, we collect more data than we actually need. By examining the initial dataset, we can gain a deeper understanding of the relationships between variables and our target variable of interest â€“ in this case, price. For this, we will use a sample dataset that contains more features than we ended up using. It's often not so much about deciding what data to collect but rather what data to retain. First, we dive into the initial data collected to examine the features that are commonly shared among most ads. After identifying these common attributes, we can optimize our data collection process by keeping these key characteristics and removing the less common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060aa9a-8c8c-4251-b75c-a02b9f7e552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    Path.cwd().joinpath(\"data\").joinpath(\"2023-10-01_Processed_dataset_for_NB_use.gzip\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0fcd9-afa2-4b7e-9c7f-85e4d3866c0c",
   "metadata": {},
   "source": [
    "As depicted in @fig-fig1, the features `energy_class`, `lat`, `lng` demonstrate the highest completeness, with more than 90% of instances being present. In contrast, `subdivision_permit`, `yearly_theoretical_energy_consumption` and `width_of_the_total_lot` are among the least populated features, with roughly 10-15% of non-missing instances.\n",
    "\n",
    "This information allows us to devise a strategy where we, for example, could retain features with a completeness of over 50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801d324-b891-43cf-aae5-b23e255673e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: \"Top 50 Features with Non-Missing Values Above 50%\"\n",
    "# | label: fig-fig1\n",
    "\n",
    "\n",
    "# Getting the column names with lowest missing values\n",
    "lowest_missing_value_columns = (\n",
    "    df.notna()\n",
    "    .sum()\n",
    "    .div(df.shape[0])\n",
    "    .mul(100)\n",
    "    .sort_values(ascending=False)\n",
    "    .head(50)\n",
    "    .round(1)\n",
    ")\n",
    "indexes_to_keep = lowest_missing_value_columns.index\n",
    "\n",
    "(\n",
    "    lowest_missing_value_columns.reset_index()\n",
    "    .rename(columns={\"index\": \"column\", 0: \"perc_values_present\"})\n",
    "    .assign(\n",
    "        Has_non_missing_values_above_50_pct=lambda df: df.perc_values_present.gt(50),\n",
    "        perc_values_present=lambda df: df.perc_values_present - 50,\n",
    "    )\n",
    "    .pipe(\n",
    "        lambda df: ggplot(\n",
    "            df,\n",
    "            aes(\n",
    "                \"perc_values_present\",\n",
    "                \"column\",\n",
    "                fill=\"Has_non_missing_values_above_50_pct\",\n",
    "            ),\n",
    "        )\n",
    "        + geom_bar(stat=\"identity\", orientation=\"y\", show_legend=False)\n",
    "        + ggsize(800, 1000)\n",
    "        + labs(\n",
    "            title=\"Top 50 Features with Non-Missing Values Above 50%\",\n",
    "            subtitle=\"\"\"The plot illustrates that the features such as'energy class,' 'lng' and 'lat' exhibited the \n",
    "            highest completeness, with over 90% of instances present. Conversely, 'subdivision_permit', was among \n",
    "            the least populated features, with approximately 10% of non-missing instances.\n",
    "            \"\"\",\n",
    "            x=\"Percentage of Instances Present with Reference Point at 50%\",\n",
    "            y=\"\",\n",
    "        )\n",
    "        + theme(\n",
    "            plot_subtitle=element_text(size=12, face=\"italic\"),\n",
    "            plot_title=element_text(size=15, face=\"bold\"),\n",
    "        )\n",
    "        + ggsize(1000, 600)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577041de-d5a8-42a2-9e98-3a39b7f9d2fd",
   "metadata": {},
   "source": [
    "## Assessing Feature Cardinality\n",
    "\n",
    "Now, let's assess the feature cardinality of our dataset to differentiate between categorical and numerical variables. To do this, we will analyze the percentage of unique values per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08131dc9-e502-4d79-b98a-9eb187fd7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: \"Assessing Feature Cardinality: Percentage of Unique Values per Feature\"\n",
    "# | label: fig-fig2\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "number_unique_entries = {\n",
    "    \"column_name\": df.columns.tolist(),\n",
    "    \"column_dtype\": [df[col].dtype for col in df.columns],\n",
    "    \"unique_values_pct\": [df[col].nunique() for col in df.columns],\n",
    "}\n",
    "\n",
    "(\n",
    "    pd.DataFrame(number_unique_entries)\n",
    "    .sort_values(\"unique_values_pct\")\n",
    "    .assign(\n",
    "        unique_values_pct=lambda x: x.unique_values_pct.div(df.shape[0])\n",
    "        .mul(100)\n",
    "        .round(1)\n",
    "    )\n",
    "    .pipe(\n",
    "        lambda df: ggplot(df, aes(\"unique_values_pct\", \"column_name\"))\n",
    "        + geom_bar(stat=\"identity\", orientation=\"y\")\n",
    "        + labs(\n",
    "            title=\"Assessing Feature Cardinality\",\n",
    "            subtitle=\"\"\" Features with a Low Cardinality (Less than 10 Distinct Values) Can Be Used as Categorical Variables, \n",
    "            while Those with Higher Cardinality, typically represented as floats or ints, May Be Used as They Are\n",
    "            \"\"\",\n",
    "            x=\"Percentage of Unique Values per Feature\",\n",
    "            y=\"\",\n",
    "        )\n",
    "        + theme(\n",
    "            plot_subtitle=element_text(\n",
    "                size=12, face=\"italic\"\n",
    "            ),  # Customize subtitle appearance\n",
    "            plot_title=element_text(size=15, face=\"bold\"),  # Customize title appearance\n",
    "        )\n",
    "        + ggsize(800, 1000)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477f8f6-2802-4645-890c-45586f08fa99",
   "metadata": {},
   "source": [
    "## Distribution of the target variable\n",
    "\n",
    "Upon examining the distribution of our target variable, which is the `price`, it becomes evident that there is a notable skew. Our median price stands at 379,000 EUR, with the lowest at 350,000 EUR and the highest reaching 10 million EUR. To increase the accuracy of our predictions, it is worth considering a transformation of our target variable before proceeding with modeling. This transformation serves several beneficial purposes:\n",
    "\n",
    "1. **Normalization**: It has the potential to render the distribution of the target variable more symmetrical, resembling a normal distribution. Such a transformation can significantly enhance the performance of various regression models.\n",
    "\n",
    "2. **Equalizing Variance**: By stabilizing the variance of the target variable across different price ranges, this transformation becomes particularly valuable for ensuring the effectiveness of certain regression algorithms.\n",
    "\n",
    "3. **Mitigating Outliers**: It is effective at diminishing the impact of extreme outliers, bolstering the model's robustness against data anomalies.\n",
    "\n",
    "4. **Interpretability**: Notably, when interpreting model predictions, this transformation allows for straightforward back-transformation to the original scale. This can be achieved using a base 10 exponentiation, ensuring that predictions are easily interpretable in their origination task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09953320-206e-4e8b-afba-87b2082ea946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: \"Target distribution before and after log10 transformation\"\n",
    "# | label: fig-fig3\n",
    "\n",
    "before_transformation = df.pipe(\n",
    "    lambda df: ggplot(df, aes(\"price\")) + geom_histogram()\n",
    ") + labs(\n",
    "    title=\"Before Transformation\",\n",
    ")\n",
    "after_transformation = df.assign(price=lambda df: np.log10(df.price)).pipe(\n",
    "    lambda df: ggplot(df, aes(\"price\"))\n",
    "    + geom_histogram()\n",
    "    + labs(\n",
    "        title=\"After log10 Transformation\",\n",
    "    )\n",
    ")\n",
    "gggrid([before_transformation, after_transformation], ncol=2) + ggsize(800, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae42bd-0a9b-44e9-a591-5a9f551a2deb",
   "metadata": {},
   "source": [
    "## Relationship between independent and dependent variables\n",
    "\n",
    "Next, we will investigate how house prices vary when grouped according to our independent variables. Please take into account that the price values have undergone log transformation to address skewness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b4d94-f938-438c-8869-1af15690ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_features = (\n",
    "    pd.DataFrame(number_unique_entries)\n",
    "    .query(\"unique_values_pct <= 5\")\n",
    "    .column_name.to_list()\n",
    ")\n",
    "\n",
    "high_cardinality_features = (\n",
    "    pd.DataFrame(number_unique_entries)\n",
    "    .query(\"(unique_values_pct >= 5)\")\n",
    "    .loc[lambda df: (df.column_dtype == \"float32\") | (df.column_dtype == \"float64\"), :]\n",
    "    .column_name.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc620a98-b613-4ff8-8eba-9bdf78bb6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: \"Exploring Price Variations Across Different Variables\"\n",
    "# | label: fig-fig4\n",
    "\n",
    "plots = []\n",
    "\n",
    "for idx, feature in enumerate(low_cardinality_features):\n",
    "    plot = (\n",
    "        df.melt(id_vars=[\"price\"])\n",
    "        .loc[lambda df: df.variable == feature, :]\n",
    "        .assign(price=lambda df: np.log10(df.price))\n",
    "        .pipe(\n",
    "            lambda df: ggplot(\n",
    "                df,\n",
    "                aes(as_discrete(\"value\"), \"price\"),\n",
    "            )\n",
    "            + facet_wrap(\"variable\")\n",
    "            + geom_boxplot(\n",
    "                show_legend=False,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    plots.append(plot)\n",
    "gggrid(plots, ncol=4) + ggsize(900, 1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d3ff7-f844-45e0-a242-57e21a4a4c48",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "Finally, we will look into the correlations among variables with high cardinality through Spearman correlation analysis. As evident from the heatmap, the price exhibits a strong correlation with cadastral income (correlation coefficient = 0.77), living area (correlation coefficient = 0.74), and bathrooms (correlation coefficient = 0.59). For your reference, _cadastral income_ is an annual Flemish property tax based on the assessed rental income of immovable properties in the Flemish Region. This income is a notional rental income assigned to each property, whether it is rented out or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7907b-abdd-4869-bb2d-4a682285534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: \"Spearman Correlations Among High Cardinality Features\"\n",
    "# | label: fig-fig5\n",
    "\n",
    "(\n",
    "    df.loc[:, lambda df: df.columns.isin(high_cardinality_features)]\n",
    "    .corr(method=\"spearman\")\n",
    "    .pipe(\n",
    "        lambda df: corr_plot(df)\n",
    "        .tiles(\n",
    "            \"lower\",\n",
    "        )\n",
    "        .labels(type=\"lower\", map_size=False)\n",
    "        .palette_gradient(low=\"#2986cc\", mid=\"#ffffff\", high=\"#d73027\")\n",
    "        .build()\n",
    "        + ggsize(900, 900)\n",
    "        + labs(\n",
    "            title=\"Spearman Correlations Among High Cardinality Features\",\n",
    "            subtitle=\"\"\" The price demonstrates robust correlations with key factors, including cadastral income (correlation coefficient = 0.77), \n",
    "            living area (correlation coefficient = 0.74), and bathrooms (correlation coefficient = 0.59)\n",
    "            \"\"\",\n",
    "            x=\"Number of Unique Values per Feature\",\n",
    "            y=\"\",\n",
    "        )\n",
    "        + theme(\n",
    "            plot_subtitle=element_text(\n",
    "                size=12, face=\"italic\"\n",
    "            ),  # Customize subtitle appearance\n",
    "            plot_title=element_text(size=15, face=\"bold\"),  # Customize title appearance\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a536dd2-e1b6-45e6-a6fc-5f90306fc789",
   "metadata": {},
   "source": [
    "Having laid the groundwork with initial data exploration in Part 1, we're now ready to take the next step: building a foundational machine learning model. In Part 2, we'll put various algorithms to the test, establishing a benchmark that will serve as a reference point for our future model-building and feature engineering efforts. This baseline model will provide a crucial starting point, guiding us as we work to refine and enhance our predictive capabilities. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
