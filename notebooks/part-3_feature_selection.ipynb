{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ecb309f7",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Peek My Home Price Part-3: Feature selection'\n",
    "author: Adam Cseresznye\n",
    "date: '2024-11-22'\n",
    "categories:\n",
    "  - Belgian Housing Market Insights\n",
    "jupyter: python3\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5a692-e95d-4785-8d85-ed1cc10b4056",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Photo by Christian Allard UnSplash](https://images.unsplash.com/photo-1549407408-4b016f497e4d?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D){fig-align=\"center\" width=50%}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0bad6-b441-4efe-b37b-a75df3552795",
   "metadata": {},
   "source": [
    "In Part 2, we looked into some crucial sample pre-processing steps before modeling, establishing the required pipeline for data processing, evaluating various algorithms, and ultimately identifying an appropriate baseline model, that is CatBoost. As we proceed to Part 3, our focus will be on assessing the significance of features in the initial scraped dataset. We'll achieve this by employing the `feature_importances_` method of CatBoostRegressor and analyzing SHAP values. Additionally, we'll systematically eliminate features showing lower importance or predictive capability. Excited to explore all these!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fca25e-3c06-4c51-834d-0e393f2dce47",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "You can explore the project's app on its [website](https://peek-my-home-price.fly.dev/). For more details, visit the [GitHub repository](https://github.com/adamcseresznye/peek_my_home_price).\n",
    "\n",
    "Check out the series for a deeper dive:\n",
    "- [Part 1: Characterizing the Data](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-1_characterizing_the_data.html)\n",
    "- [Part 2: Building a Baseline Model](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-2_building_a_baseline_model.html)\n",
    "- [Part 3: Feature Selection](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-3_feature_selection.html)\n",
    "- [Part 4: Feature Engineering](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-4_feature_engineering.html)\n",
    "- [Part 5: Fine-Tuning](https://adamcseresznye.github.io/blog/projects/peek_my_home_price/part-5_fine_tuning.html)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d1f9e-38f5-4963-8715-f3ba1f8ed5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c6a54-530c-4491-bb08-3dfc054d78a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import catboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from IPython.display import clear_output\n",
    "from lets_plot import *\n",
    "from lets_plot.mapping import as_discrete\n",
    "from sklearn import metrics, model_selection\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helper import utils\n",
    "\n",
    "LetsPlot.setup_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86846c91-1d46-4c2e-8fb2-72f5c5b8e8fd",
   "metadata": {},
   "source": [
    "# Prepare dataframe before modelling\n",
    "## Read in dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6ba3f-01d0-4f8d-9a08-1c3d99c01210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    Path.cwd().joinpath(\"data\").joinpath(\"2023-10-01_Processed_dataset_for_NB_use.gzip\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f6be7-d5e3-48c7-95ec-c3175589f97e",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "Our initial step involves a train-test split. This process divides the data into two distinct sets: a training set and a testing set. The training set is employed for model training, while the testing set is exclusively reserved for model evaluation. This methodology allows models to be trained on the training set and then assessed for accuracy using the unseen testing set. Ultimately, this approach enables an unbiased evaluation of our model's performance, utilizing the test set that remained untouched during the model training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59bd8a-87cd-4850-a48c-b660ed897135",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(\n",
    "    df, test_size=0.2, random_state=utils.Configuration.seed\n",
    ")\n",
    "\n",
    "print(f\"Shape of train: {train.shape}\")\n",
    "print(f\"Shape of test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b001db1c-74f3-465c-9a21-907c3101e7bb",
   "metadata": {},
   "source": [
    "## Preprocess dataframe for modelling\n",
    "\n",
    "Next, we'll apply a crucial step in our data preprocessing pipeline: transforming our target variable using a logarithmic function. Additionally, we'll tackle missing values in our categorical features by replacing them with a designated label, \"missing value.\" This is needed, as CatBoost can handle missing values in numerical columns, but categorical missing values require manual attention to ensure accurate modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698661bc-c89a-445f-b989-694acece58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = train.reset_index(drop=True).assign(\n",
    "    price=lambda df: np.log10(df.price)\n",
    ")  # Log transformation of 'price' column\n",
    "\n",
    "# This step is needed since catboost cannot handle missing values when feature is categorical\n",
    "for col in processed_train.columns:\n",
    "    if processed_train[col].dtype.name in (\"bool\", \"object\", \"category\"):\n",
    "        processed_train[col] = processed_train[col].fillna(\"missing value\")\n",
    "\n",
    "processed_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ed576-57ce-47d1-85e3-ab45913c8104",
   "metadata": {},
   "source": [
    "# Inspect feature importance\n",
    "\n",
    "We will evaluate feature importance using two methods: the `feature_importances_` attribute in CatBoost and SHAP values from the SHAP library. To begin examining feature importances, we'll initiate model training. This involves further partitioning the training set, reserving a portion for CatBoost training (validation dataset). This allows us to stop the training process when overfitting emerges. Preventing overfitting is crucial, as it ensures we don't work with an overly biased model. Additionally, if overfitting occurs, stopping training earlier helps conserve time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4d3df-d7fd-4baf-95a8-850f290d6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = processed_train.columns[~processed_train.columns.str.contains(\"price\")]\n",
    "\n",
    "numerical_features = processed_train.select_dtypes(\"number\").columns.to_list()\n",
    "categorical_features = processed_train.select_dtypes(\"object\").columns.to_list()\n",
    "\n",
    "train_FS, validation_FS = model_selection.train_test_split(\n",
    "    processed_train, test_size=0.2, random_state=utils.Configuration.seed\n",
    ")\n",
    "\n",
    "# Get target variables\n",
    "tr_y = train_FS[utils.Configuration.target_col]\n",
    "val_y = validation_FS[utils.Configuration.target_col]\n",
    "\n",
    "# Get feature matrices\n",
    "tr_X = train_FS.loc[:, features]\n",
    "val_X = validation_FS.loc[:, features]\n",
    "\n",
    "\n",
    "print(f\"Train dataset shape: {tr_X.shape} {tr_y.shape}\")\n",
    "print(f\"Validation dataset shape: {val_X.shape} {val_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72276cf9-97c9-4c1e-8be9-8990c1f57a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = catboost.Pool(tr_X, tr_y, cat_features=categorical_features)\n",
    "validation_dataset = catboost.Pool(val_X, val_y, cat_features=categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0593e4a5-8973-474c-891c-5db866de39c6",
   "metadata": {},
   "source": [
    "As you can see below, the training loss steadily decreases, but the validation loss reaches a plateau. Thanks to the validation dataset, we can stop the training well before the initially set 2000 iterations. This early stopping is crucial for preventing overfitting and ensures a more balanced and effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de9cc0-9a41-483f-a6a8-45c4e9b1bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = catboost.CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    random_seed=utils.Configuration.seed,\n",
    "    loss_function=\"RMSE\",\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    eval_set=[validation_dataset],\n",
    "    early_stopping_rounds=20,\n",
    "    use_best_model=True,\n",
    "    verbose=2000,\n",
    "    plot=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f656e21-1487-44f0-ae06-0457dcf69f48",
   "metadata": {},
   "source": [
    "According to our trained CatBoost model, the most significant feature in our dataset is the living_area, followed by cadastral_income and latitude. To validate and compare these findings, we'll examine the SHAP values to understand how they align or differ from the feature importances provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a60279-f094-41d3-873a-60050ac5660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: \"Assessing Feature Importance\"\n",
    "# | label: fig-fig1\n",
    "\n",
    "(\n",
    "    pd.concat(\n",
    "        [pd.Series(model.feature_names_), pd.Series(model.feature_importances_)], axis=1\n",
    "    )\n",
    "    .sort_values(by=1, ascending=False)\n",
    "    .rename(columns={0: \"name\", 1: \"importance\"})\n",
    "    .reset_index(drop=True)\n",
    "    .pipe(\n",
    "        lambda df: ggplot(df, aes(\"name\", \"importance\"))\n",
    "        + geom_bar(stat=\"identity\")\n",
    "        + labs(\n",
    "            title=\"Assessing Feature Importance\",\n",
    "            subtitle=\"\"\" based on the feature_importances_ attribute\n",
    "            \"\"\",\n",
    "            x=\"\",\n",
    "            y=\"Feature Importance\",\n",
    "        )\n",
    "        + theme(\n",
    "            plot_subtitle=element_text(\n",
    "                size=12, face=\"italic\"\n",
    "            ),  # Customize subtitle appearance\n",
    "            plot_title=element_text(size=15, face=\"bold\"),  # Customize title appearance\n",
    "        )\n",
    "        + ggsize(800, 600)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9c0b6-22ba-49fe-a2ca-c386ecb1055e",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values offer a method to understand the predictions made by any machine learning model. These values leverage a game-theoretic approach to quantify the contribution of each \"player\" (feature) to the final prediction. In the realm of machine learning, SHAP values assign an importance value to each feature, delineating its contribution to the model's output.\r\n",
    "\r\n",
    "SHAP values provide detailed insights into how each feature influences individual predictions, their relative significance in comparison to one another, and the model's reliance on interactions between features. This comprehensive perspective enables a deeper understanding of the factors that drive the model's decision-making process.\r\n",
    "\r\n",
    "In this phase, our primary focus will involve computing SHAP values and then creating visualizations, such as bar plots and beeswarm plots, to illustrate feature importance and interactionss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908de2af-8ca3-48fe-961b-e9ad1687a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(\n",
    "    catboost.Pool(tr_X, tr_y, cat_features=categorical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb939649-ebe8-4bad-b70b-79e5e0ee2591",
   "metadata": {},
   "source": [
    "The summary plot at @fig-fig2 provides a clear depiction of feature importance within the model. The outcomes reveal that \"Living area,\" \"Surface of the plot,\" and \"Cadastral income\" emerge as important factors in influencing the model's predictions. These features prominently contribute to determining the model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4664de-015e-482c-86e3-e44a7f1789a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: \"Assessing Feature Importance using bar plot\"\n",
    "# | label: fig-fig2\n",
    "\n",
    "shap.summary_plot(shap_values, tr_X, plot_type=\"bar\", plot_size=[11, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6254e0-e259-4a57-96df-008f7ef97fa7",
   "metadata": {},
   "source": [
    "The beeswarm plot (@fig-fig3) can be best understood through the following breakdown:\r\n",
    "\r\n",
    "- The Y-axis arranges the feature names based on their importance, with the most influential features placed at the top.\r\n",
    "- The X-axis displays the SHAP value, representing the impact of each feature on the model's output. Features on the right side of the plot exhibit a stronger impact, while those on the left possess a weaker influence.\r\n",
    "\r\n",
    "Moreover:\r\n",
    "- Each point's color on the plot signifies the respective feature's value for that specific data point. Red indicates high values, while blue represents low values.\r\n",
    "- Every individual point on the plot corresponds to a particular row from the original dataset. Collectively, these points illustrate how different data points and their associated feature values contribute to the model's predictions, especially concerning feature importance.\r\n",
    "\r\n",
    "Upon examining the \"living area\" feature, you'll notice it predominantly displays a high positive SHAP value. This suggests that a larger living area tends to have a positive effect on the output, which in this context is the price. Conversely, higher values of \"primary energy consumption\" are associated with a negative impact on the price, reflected by their negative SHAP values.\r\n",
    "\r\n",
    "Consideration of the spread of SHAP values and their relation to predictive power is important. A wider spread or a denser distribution of data points implies greater variability or a more significant influence on the model's predictions. This insight allows us to evaluate the significance of features regarding their contribution to the model's overall output.\r\n",
    "\r\n",
    "This context clarifies why \"living area\" holds greater importance compared to \"CO2 emission.\" The broader impact and higher variability of the \"living area\" feature in influencing the model's predictions make it a more crucial determinant of the output, thus carrying more weight in the model's decision-making process.rtance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba141762-f6aa-4071-8366-9388e5083586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: \"Assessing Feature Importance using beeswarm plot\"\n",
    "# | label: fig-fig3\n",
    "\n",
    "shap.summary_plot(shap_values, tr_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1a276-1d71-4628-9add-9eeea2f37a32",
   "metadata": {},
   "source": [
    "Let's examine the ranking or order of feature importances derived from both Gini impurity and SHAP values to understand how they compare and whether they yield similar or differing insights. As you can see from the table below, they are fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf2bf6-b968-400e-a243-36de608d42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_feature_importance = (\n",
    "    pd.concat(\n",
    "        [pd.Series(model.feature_names_), pd.Series(model.feature_importances_)], axis=1\n",
    "    )\n",
    "    .sort_values(by=1, ascending=False)\n",
    "    .rename(columns={0: \"catboost_name\", 1: \"importance\"})\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad37f61-3c5f-4345-9a3f-7949e88ae8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_feature_importance = (\n",
    "    pd.DataFrame(shap_values, columns=tr_X.columns)\n",
    "    .abs()\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"shap_name\", 0: \"shap\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b09e1-a6c4-49d4-8385-4212d35c3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        catboost_feature_importance.drop(columns=\"importance\"),\n",
    "        shap_feature_importance.drop(columns=\"shap\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36b785-cf35-4a33-b9ad-802a2d5da128",
   "metadata": {},
   "source": [
    "# Recursive feature elimination based on SHAP values\n",
    "\n",
    "Next, we'll work on the initial feature elimination process based on SHAP values using CatBoost's `select_features` method. Although a rich set of features can be advantageous, the quest for model interpretability prompts us to consider the need for a streamlined feature set.\n",
    "\n",
    "Our objective here is to remove features that have minimal impact on the final predictive output, retaining only the most influential ones. This action streamlines our model, enhancing its interpretability and making it easier to understand the factors driving its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211701ba-1a83-445e-ab12-a36023e728d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = catboost.CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    cat_features=categorical_features,\n",
    "    random_seed=utils.Configuration.seed,\n",
    "    loss_function=\"RMSE\",\n",
    ")\n",
    "\n",
    "rfe_dict = regressor.select_features(\n",
    "    algorithm=\"RecursiveByShapValues\",\n",
    "    shap_calc_type=\"Exact\",\n",
    "    X=tr_X,\n",
    "    y=tr_y,\n",
    "    eval_set=(val_X, val_y),\n",
    "    features_for_select=\"0-48\",\n",
    "    num_features_to_select=1,\n",
    "    steps=10,\n",
    "    verbose=2000,\n",
    "    train_final_model=False,\n",
    "    plot=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29708d2-58ae-46b2-9e88-3134f8a0133c",
   "metadata": {},
   "source": [
    "Through Recursive Feature Elimination, we've successfully decreased the number of features from an initial count of 49 to a more concise set of 17, up to and including the \"bedrooms\" feature. This reduction in features hasn't notably affected our model's performance, enabling us to retain a comparable level of predictive accuracy. This streamlined dataset enhances the model's simplicity and interpretability without compromising its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c302754-9a56-4d48-ae5d-d296ad5e4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = (\n",
    "    rfe_dict[\"eliminated_features_names\"][33:] + rfe_dict[\"selected_features_names\"]\n",
    ")\n",
    "\n",
    "print(features_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19912064-c2ae-4416-b564-e5bf3c71cdac",
   "metadata": {},
   "source": [
    "That's it for now. In the next part, our focus will be on identifying potential outliers within our dataset. \n",
    "Additionally, we'll dive into several further feature engineering steps aimed at increasing our model's performance. For this, we will use cross-validation, ensuring the robustness and reliability of our modes. Looking forward to the next steps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
